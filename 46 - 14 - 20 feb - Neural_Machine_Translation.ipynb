{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohRHuWWWyjnk",
        "outputId": "f0421695-23ad-43aa-c167-4b1f1940248e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puGOCWIgzBHp",
        "outputId": "6326b47a-328b-45de-84cf-3296f369b759"
      },
      "source": [
        "cd /content/drive/MyDrive/ML DL Assignment and Quizzes/Deep Learning/DL Notes/DL Phase 3 Codes"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/11fup9dLOHGdLYHRf3LudO-ZRG9B97O1b/ML DL Assignment and Quizzes/Deep Learning/DL Notes/DL Phase 3 Codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "About the Dataset:\n",
        "- It contains english phrases and their corresponding german translations"
      ],
      "metadata": {
        "id": "XCkjUeFhn6pe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBvRP46XzDQW"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nreoWAG5JL4g"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        " # The file is opened in UTF-8 as ASCII mode does not have representation for\n",
        " # some german characters.\n",
        "\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEaqKO9lJPUE"
      },
      "source": [
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYX4LasrJSJs"
      },
      "source": [
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters by converting all of them to ASCII\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`re_print = re.compile('[^%s]' % re.escape(string.printable))`\n",
        "<br>\n",
        "- It creates a regular expression pattern object that matches any character that is not a printable ASCII character. The re.compile() function compiles the regular expression pattern into a regular expression object that can be used to search for non-printable characters in a string.\n",
        "\n",
        "- The string.printable constant is a string of all ASCII characters considered printable. The % operator is used to substitute the string.printable constant into the regular expression pattern. The re.escape() function is used to escape any special characters in the string.printable constant so that they are treated as literal characters in the regular expression pattern.\n",
        "\n",
        "- The resulting regular expression pattern object, re_print, can be used with the re.search() or re.sub() functions to search for or replace non-printable characters in a string."
      ],
      "metadata": {
        "id": "lnaf6zMF2cbP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "par33x-OJUcD"
      },
      "source": [
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK_p6AzTJW1R",
        "outputId": "19bc19e3-b307-41d0-bf9f-11c15a498757"
      },
      "source": [
        "# load dataset\n",
        "filename = 'deu.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-german.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german.pkl\n",
            "[hi] => [hallo]\n",
            "[hi] => [gru gott]\n",
            "[run] => [lauf]\n",
            "[wow] => [potzdonner]\n",
            "[wow] => [donnerwetter]\n",
            "[fire] => [feuer]\n",
            "[help] => [hilfe]\n",
            "[help] => [zu hulf]\n",
            "[stop] => [stopp]\n",
            "[wait] => [warte]\n",
            "[hello] => [hallo]\n",
            "[i try] => [ich probiere es]\n",
            "[i won] => [ich hab gewonnen]\n",
            "[i won] => [ich habe gewonnen]\n",
            "[smile] => [lacheln]\n",
            "[cheers] => [zum wohl]\n",
            "[freeze] => [keine bewegung]\n",
            "[freeze] => [stehenbleiben]\n",
            "[got it] => [verstanden]\n",
            "[got it] => [einverstanden]\n",
            "[he ran] => [er rannte]\n",
            "[he ran] => [er lief]\n",
            "[hop in] => [mach mit]\n",
            "[hug me] => [druck mich]\n",
            "[hug me] => [nimm mich in den arm]\n",
            "[hug me] => [umarme mich]\n",
            "[i fell] => [ich fiel]\n",
            "[i fell] => [ich fiel hin]\n",
            "[i fell] => [ich sturzte]\n",
            "[i fell] => [ich bin hingefallen]\n",
            "[i fell] => [ich bin gesturzt]\n",
            "[i know] => [ich wei]\n",
            "[i lied] => [ich habe gelogen]\n",
            "[i lost] => [ich habe verloren]\n",
            "[im] => [ich bin jahre alt]\n",
            "[im] => [ich bin]\n",
            "[im ok] => [mir gehts gut]\n",
            "[im ok] => [es geht mir gut]\n",
            "[no way] => [unmoglich]\n",
            "[no way] => [das gibts doch nicht]\n",
            "[no way] => [ausgeschlossen]\n",
            "[no way] => [in keinster weise]\n",
            "[really] => [wirklich]\n",
            "[really] => [echt]\n",
            "[really] => [im ernst]\n",
            "[thanks] => [danke]\n",
            "[try it] => [versuchs]\n",
            "[why me] => [warum ich]\n",
            "[ask tom] => [frag tom]\n",
            "[ask tom] => [fragen sie tom]\n",
            "[ask tom] => [fragt tom]\n",
            "[be cool] => [entspann dich]\n",
            "[be fair] => [sei nicht ungerecht]\n",
            "[be fair] => [sei fair]\n",
            "[be nice] => [sei nett]\n",
            "[be nice] => [seien sie nett]\n",
            "[beat it] => [geh weg]\n",
            "[beat it] => [hau ab]\n",
            "[beat it] => [verschwinde]\n",
            "[beat it] => [verdufte]\n",
            "[beat it] => [mach dich fort]\n",
            "[beat it] => [zieh leine]\n",
            "[beat it] => [mach dich vom acker]\n",
            "[beat it] => [verzieh dich]\n",
            "[beat it] => [verkrumele dich]\n",
            "[beat it] => [troll dich]\n",
            "[beat it] => [zisch ab]\n",
            "[beat it] => [pack dich]\n",
            "[beat it] => [mach ne fliege]\n",
            "[beat it] => [schwirr ab]\n",
            "[beat it] => [mach die sause]\n",
            "[beat it] => [scher dich weg]\n",
            "[beat it] => [scher dich fort]\n",
            "[call me] => [ruf mich an]\n",
            "[come in] => [komm herein]\n",
            "[come in] => [herein]\n",
            "[come on] => [komm]\n",
            "[come on] => [kommt]\n",
            "[come on] => [mach schon]\n",
            "[come on] => [macht schon]\n",
            "[get out] => [raus]\n",
            "[go away] => [geh weg]\n",
            "[go away] => [hau ab]\n",
            "[go away] => [verschwinde]\n",
            "[go away] => [verdufte]\n",
            "[go away] => [mach dich fort]\n",
            "[go away] => [zieh leine]\n",
            "[go away] => [mach dich vom acker]\n",
            "[go away] => [verzieh dich]\n",
            "[go away] => [verkrumele dich]\n",
            "[go away] => [troll dich]\n",
            "[go away] => [zisch ab]\n",
            "[go away] => [pack dich]\n",
            "[go away] => [mach ne fliege]\n",
            "[go away] => [schwirr ab]\n",
            "[go away] => [mach die sause]\n",
            "[go away] => [scher dich weg]\n",
            "[go away] => [scher dich fort]\n",
            "[go away] => [geh weg]\n",
            "[go away] => [verpiss dich]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zBLfzQXJY5o"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psa1f-ZRJshT"
      },
      "source": [
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lM2sgRgJuhU",
        "outputId": "2c2cf131-3748-4408-80c0-776520d83fd8"
      },
      "source": [
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-german.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "# selecting sentences using slicing\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[9000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-german-both.pkl')\n",
        "save_clean_data(train, 'english-german-train.pkl')\n",
        "save_clean_data(test, 'english-german-test.pkl')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german-both.pkl\n",
            "Saved: english-german-train.pkl\n",
            "Saved: english-german-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPW-i_9XJw83"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igyVRvoUJ9x9"
      },
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbmKMnvyKcuF"
      },
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "# tokenizer will associate each word to a number and convert the text to\n",
        "# vector. Now once this is done any sentence which comes in future will be\n",
        "# converted to vector in the same fashion as we have the numerical representation\n",
        "# of the vocabulary.\n",
        "# Ensure that furture sentences do not have any out of vocabulary words\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7-fW0AsKe3Z"
      },
      "source": [
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlLFK9UcKgwx"
      },
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# input is a tokenizer, maxium length of sentence in that language, and lines\n",
        "\t# of that language\n",
        "\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        " # converting list of sentences to list of vectors\n",
        "\n",
        "\t# pad sequences with 0 values as once we know the maximum length of the sentence\n",
        "\t# we know in translating it wont go beyond it\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        " # post means the zeroes will be padded after the sentence\n",
        "\treturn X\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_GjqK4bKi7j"
      },
      "source": [
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units)`\n",
        "- src_vocab: germany vocabulary size\n",
        "- tar_vocab: english vocabulary size\n",
        "- src_timesteps: maximum length of sentence in germany\n",
        "- tar_timesteps: maximum length of sentence in english\n",
        "- n_units: number of neurons in the LSTM layer"
      ],
      "metadata": {
        "id": "gK72_u3ffIlc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqy4ncCdKlNm"
      },
      "source": [
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps))\n",
        " # you can use word2vec or glove embeddings also\n",
        "\n",
        "\tmodel.add(LSTM(n_units)) # 1 layers of LSTM for encoder\n",
        "\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        " # here we are passing the encoded information of encoder to all timesteps of\n",
        " # decoder, by repeating the last time step of encoder for tar_timestep number of\n",
        " # times, and put the LSTM decoder on top of it. Now these repeated time steps of\n",
        " # encoder are passed to the timesteps of decoder correspondingly\n",
        "\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True)) # LSTM for decoder\n",
        "\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\t# here we are un rolling the decoder LSTM in time, and adding a dense layer with\n",
        "\t# number of neurons equal to number of words in english so that encoder and\n",
        "\t# decoder can be connected.\n",
        "\n",
        "\treturn model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiL6S3r1Knrq"
      },
      "source": [
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnwkjO3MKqd3",
        "outputId": "f3447d96-2477-423f-c3d6-253bbb82a8df"
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "# we are tokenizing only the english words\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1 # +1 is for <START>\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# to find the maximum length of an english sentence\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "# we are tokenizing only the german words\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 2404\n",
            "English Max Length: 5\n",
            "German Vocabulary Size: 3856\n",
            "German Max Length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSGbgzbaKsfV",
        "outputId": "edc16f80-3d46-4681-ec7f-b30df569e6f2"
      },
      "source": [
        "# prepare training data to translate from german to english\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "print(trainY[0])\n",
        "\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# encoding output as one hot encoded vectors\n",
        "\n",
        "\n",
        "print(trainY[0])\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 10   3 188  31   0]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)`\n",
        "- ger_vocab_size: germany vocabulary size\n",
        "- eng_vocab_size: english vocabulary size\n",
        "- ger_length: maximum length of sentence in germany\n",
        "- eng_length: maximum length of sentence in english\n",
        "- 256: number of neurons in the LSTM layer"
      ],
      "metadata": {
        "id": "QdFWVoTNeP3-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X4LQ7lvKwjS",
        "outputId": "647c59ae-dadc-4d4a-f187-0beeb5a999db"
      },
      "source": [
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "#plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 10, 256)           987136    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector (RepeatVecto  (None, 5, 256)            0         \n",
            " r)                                                              \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 5, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 5, 2404)           617828    \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2655588 (10.13 MB)\n",
            "Trainable params: 2655588 (10.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmlJBQJqKzeM",
        "outputId": "dbde38a8-1671-4c59-c710-c19df9a5b665"
      },
      "source": [
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=15, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 3.79469, saving model to model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141/141 - 31s - loss: 4.2881 - val_loss: 3.7947 - 31s/epoch - 222ms/step\n",
            "Epoch 2/15\n",
            "\n",
            "Epoch 2: val_loss improved from 3.79469 to 3.69118, saving model to model.h5\n",
            "141/141 - 27s - loss: 3.6882 - val_loss: 3.6912 - 27s/epoch - 191ms/step\n",
            "Epoch 3/15\n",
            "\n",
            "Epoch 3: val_loss improved from 3.69118 to 3.63103, saving model to model.h5\n",
            "141/141 - 26s - loss: 3.6015 - val_loss: 3.6310 - 26s/epoch - 183ms/step\n",
            "Epoch 4/15\n",
            "\n",
            "Epoch 4: val_loss improved from 3.63103 to 3.53118, saving model to model.h5\n",
            "141/141 - 25s - loss: 3.4937 - val_loss: 3.5312 - 25s/epoch - 174ms/step\n",
            "Epoch 5/15\n",
            "\n",
            "Epoch 5: val_loss improved from 3.53118 to 3.46423, saving model to model.h5\n",
            "141/141 - 26s - loss: 3.3727 - val_loss: 3.4642 - 26s/epoch - 184ms/step\n",
            "Epoch 6/15\n",
            "\n",
            "Epoch 6: val_loss improved from 3.46423 to 3.42054, saving model to model.h5\n",
            "141/141 - 26s - loss: 3.2749 - val_loss: 3.4205 - 26s/epoch - 188ms/step\n",
            "Epoch 7/15\n",
            "\n",
            "Epoch 7: val_loss improved from 3.42054 to 3.34896, saving model to model.h5\n",
            "141/141 - 26s - loss: 3.1819 - val_loss: 3.3490 - 26s/epoch - 183ms/step\n",
            "Epoch 8/15\n",
            "\n",
            "Epoch 8: val_loss improved from 3.34896 to 3.24905, saving model to model.h5\n",
            "141/141 - 26s - loss: 3.0553 - val_loss: 3.2491 - 26s/epoch - 183ms/step\n",
            "Epoch 9/15\n",
            "\n",
            "Epoch 9: val_loss improved from 3.24905 to 3.15344, saving model to model.h5\n",
            "141/141 - 24s - loss: 2.9221 - val_loss: 3.1534 - 24s/epoch - 168ms/step\n",
            "Epoch 10/15\n",
            "\n",
            "Epoch 10: val_loss improved from 3.15344 to 3.08152, saving model to model.h5\n",
            "141/141 - 25s - loss: 2.7780 - val_loss: 3.0815 - 25s/epoch - 175ms/step\n",
            "Epoch 11/15\n",
            "\n",
            "Epoch 11: val_loss improved from 3.08152 to 2.97815, saving model to model.h5\n",
            "141/141 - 25s - loss: 2.6347 - val_loss: 2.9782 - 25s/epoch - 176ms/step\n",
            "Epoch 12/15\n",
            "\n",
            "Epoch 12: val_loss improved from 2.97815 to 2.87083, saving model to model.h5\n",
            "141/141 - 24s - loss: 2.4856 - val_loss: 2.8708 - 24s/epoch - 171ms/step\n",
            "Epoch 13/15\n",
            "\n",
            "Epoch 13: val_loss improved from 2.87083 to 2.77781, saving model to model.h5\n",
            "141/141 - 23s - loss: 2.3386 - val_loss: 2.7778 - 23s/epoch - 164ms/step\n",
            "Epoch 14/15\n",
            "\n",
            "Epoch 14: val_loss improved from 2.77781 to 2.72211, saving model to model.h5\n",
            "141/141 - 26s - loss: 2.2047 - val_loss: 2.7221 - 26s/epoch - 182ms/step\n",
            "Epoch 15/15\n",
            "\n",
            "Epoch 15: val_loss improved from 2.72211 to 2.64921, saving model to model.h5\n",
            "141/141 - 28s - loss: 2.0795 - val_loss: 2.6492 - 28s/epoch - 201ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7be3e85d3370>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to translate new sentence"
      ],
      "metadata": {
        "id": "__bOZbAYf3qW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXUGv5SZK-Qp",
        "outputId": "78db39ac-9e82-4feb-9344-418b31541ecb"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "src=[wohnen sie hier], target=[do you live here], predicted=[do you it]\n",
            "src=[sei objektiv], target=[be objective], predicted=[be careful]\n",
            "src=[das war laut], target=[that was loud], predicted=[she was easy]\n",
            "src=[bitte setzt euch hierhin], target=[please sit here], predicted=[please please here]\n",
            "src=[ich habe mir nichts dabei gedacht], target=[i didnt mean it], predicted=[i like to]\n",
            "src=[ich bin online], target=[i am online], predicted=[i am busy]\n",
            "src=[ich mag eiscreme], target=[i love ice cream], predicted=[i like to]\n",
            "src=[zeig sie ihm], target=[show it to him], predicted=[do it to me]\n",
            "src=[hat tom ihn gefunden], target=[did tom find it], predicted=[did tom do it]\n",
            "src=[er hat aufgelegt], target=[he hung up], predicted=[he is]\n",
            "test\n",
            "src=[tom setzte sich], target=[tom sat down], predicted=[tom will]\n",
            "src=[ich habe zu tun], target=[im busy], predicted=[i am a]\n",
            "src=[seien sie nicht unhoflich], target=[dont be rude], predicted=[dont be me]\n",
            "src=[ich bin erschopft], target=[i am exhausted], predicted=[im am]\n",
            "src=[fass mit an], target=[lend us a hand], predicted=[dont to]\n",
            "src=[ich habe tom meine stimme gegeben], target=[i voted for tom], predicted=[i cant see it]\n",
            "src=[halten sie ihn nicht auf], target=[dont stop him], predicted=[dont me me]\n",
            "src=[wie wird das enden], target=[how will it end], predicted=[how do it it]\n",
            "src=[er besitzt ubersinnliche krafte], target=[he is a psychic], predicted=[he is a a]\n",
            "src=[gehen sie in deckung], target=[take cover], predicted=[dont up]\n"
          ]
        }
      ]
    }
  ]
}